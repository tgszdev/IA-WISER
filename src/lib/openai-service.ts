// OpenAI Service - Integra√ß√£o Segura com OpenAI
import OpenAI from 'openai';

export interface OpenAIConfig {
  apiKey?: string;
  model?: string;
  temperature?: number;
  maxTokens?: number;
}

export class OpenAIService {
  private client: OpenAI | null = null;
  private model: string = 'gpt-4-turbo-preview'; // ou gpt-3.5-turbo para economizar
  private isConfigured: boolean = false;

  constructor(config?: OpenAIConfig) {
    const apiKey = config?.apiKey || 
                   process.env.OPENAI_API_KEY || 
                   '';

    if (apiKey && apiKey !== '' && !apiKey.includes('xxx')) {
      try {
        this.client = new OpenAI({
          apiKey: apiKey,
        });
        this.model = config?.model || this.model;
        this.isConfigured = true;
        console.log('‚úÖ OpenAI configurado com sucesso');
      } catch (error) {
        console.error('‚ùå Erro ao configurar OpenAI:', error);
        this.isConfigured = false;
      }
    } else {
      console.warn('‚ö†Ô∏è OpenAI API Key n√£o configurada');
    }
  }

  // Verifica se est√° configurado
  isReady(): boolean {
    return this.isConfigured && this.client !== null;
  }

  // Processa mensagem com contexto de invent√°rio
  async processInventoryQuery(
    message: string,
    inventoryData?: any,
    sessionHistory?: any[]
  ): Promise<string> {
    if (!this.isReady()) {
      return 'OpenAI n√£o est√° configurado. Use o Query Generator local.';
    }

    try {
      // Prepara o contexto do sistema
      const systemPrompt = `
Voc√™ √© o Wiser IA Assistant, especializado em gest√£o de invent√°rio.

CAPACIDADES:
- Analisar dados de produtos e estoque
- Identificar problemas (avarias, vencimentos, bloqueios)
- Calcular estat√≠sticas e tend√™ncias
- Fornecer insights acion√°veis

REGRAS:
1. Seja preciso com n√∫meros e quantidades
2. Destaque informa√ß√µes cr√≠ticas com emojis apropriados
3. Forne√ßa respostas concisas mas completas
4. Sugira pr√≥ximas a√ß√µes quando relevante
5. Use portugu√™s brasileiro

FORMATO:
- Use **negrito** para destacar
- Use emojis: ‚úÖ sucesso, ‚ö†Ô∏è alerta, ‚ùå erro, üì¶ produto, üìä estat√≠stica
- Termine com sugest√µes quando apropriado
`;

      // Prepara mensagens
      const messages: any[] = [
        { role: 'system', content: systemPrompt }
      ];

      // Adiciona hist√≥rico se dispon√≠vel
      if (sessionHistory && sessionHistory.length > 0) {
        sessionHistory.slice(-5).forEach(msg => {
          messages.push({
            role: msg.role === 'user' ? 'user' : 'assistant',
            content: msg.content
          });
        });
      }

      // Adiciona dados de invent√°rio se dispon√≠vel
      if (inventoryData) {
        messages.push({
          role: 'system',
          content: `Dados do invent√°rio dispon√≠veis: ${JSON.stringify(inventoryData).slice(0, 2000)}`
        });
      }

      // Adiciona mensagem do usu√°rio
      messages.push({ role: 'user', content: message });

      // Faz a chamada para OpenAI
      const completion = await this.client!.chat.completions.create({
        model: this.model,
        messages: messages,
        temperature: 0.3, // Mais determin√≠stico para dados
        max_tokens: 500,
        presence_penalty: 0.1,
        frequency_penalty: 0.1
      });

      return completion.choices[0]?.message?.content || 'Sem resposta';

    } catch (error: any) {
      console.error('‚ùå Erro OpenAI:', error);
      
      if (error.status === 429) {
        return '‚ö†Ô∏è Limite de requisi√ß√µes excedido. Tente novamente em alguns segundos.';
      } else if (error.status === 401) {
        return '‚ùå API Key inv√°lida. Verifique suas credenciais.';
      } else {
        return `‚ùå Erro ao processar: ${error.message}`;
      }
    }
  }

  // An√°lise de tend√™ncias
  async analyzeTrends(data: any[]): Promise<string> {
    if (!this.isReady()) {
      return 'OpenAI n√£o configurado';
    }

    const prompt = `
Analise os seguintes dados de invent√°rio e identifique:
1. Tend√™ncias principais
2. Produtos cr√≠ticos
3. Recomenda√ß√µes de a√ß√£o

Dados: ${JSON.stringify(data).slice(0, 3000)}
`;

    return this.processInventoryQuery(prompt);
  }

  // Gera√ß√£o de relat√≥rio
  async generateReport(data: any, type: string = 'summary'): Promise<string> {
    if (!this.isReady()) {
      return 'OpenAI n√£o configurado';
    }

    const prompts: any = {
      summary: 'Crie um resumo executivo destes dados de invent√°rio',
      detailed: 'Crie um relat√≥rio detalhado com an√°lises e recomenda√ß√µes',
      critical: 'Identifique apenas itens cr√≠ticos que precisam aten√ß√£o imediata'
    };

    const prompt = `${prompts[type] || prompts.summary}\n\nDados: ${JSON.stringify(data).slice(0, 3000)}`;
    
    return this.processInventoryQuery(prompt);
  }

  // Compara√ß√£o com GPT-4
  async compareWithGPT4(query: string, localResponse: string): Promise<any> {
    if (!this.isReady()) {
      return { enhanced: false, response: localResponse };
    }

    try {
      const prompt = `
Query do usu√°rio: ${query}

Resposta do sistema local:
${localResponse}

Melhore esta resposta, tornando-a mais √∫til e acion√°vel.
Mantenha a precis√£o dos dados mas adicione insights.
`;

      const enhanced = await this.processInventoryQuery(prompt);
      
      return {
        enhanced: true,
        original: localResponse,
        improved: enhanced
      };
    } catch (error) {
      return { enhanced: false, response: localResponse };
    }
  }

  // Fun√ß√£o para chat streaming (opcional)
  async *streamChat(message: string): AsyncGenerator<string> {
    if (!this.isReady()) {
      yield 'OpenAI n√£o configurado';
      return;
    }

    try {
      const stream = await this.client!.chat.completions.create({
        model: this.model,
        messages: [
          { role: 'system', content: 'Voc√™ √© um assistente de invent√°rio.' },
          { role: 'user', content: message }
        ],
        stream: true,
        temperature: 0.3
      });

      for await (const chunk of stream) {
        yield chunk.choices[0]?.delta?.content || '';
      }
    } catch (error) {
      yield `Erro: ${error}`;
    }
  }

  // Estat√≠sticas de uso
  async getUsageStats(): Promise<any> {
    // OpenAI n√£o fornece diretamente, mas voc√™ pode rastrear localmente
    return {
      model: this.model,
      configured: this.isConfigured,
      available: this.isReady()
    };
  }
}

// Singleton
let openAIService: OpenAIService | null = null;

export function getOpenAIService(config?: OpenAIConfig): OpenAIService {
  if (!openAIService) {
    openAIService = new OpenAIService(config);
  }
  return openAIService;
}